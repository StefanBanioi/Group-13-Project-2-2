{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8188, 146) (8188,)\n",
      "(8188, 146) (8188,)\n",
      "float64 float64\n",
      "<class 'numpy.ndarray'> <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#Built me pytorch ANN that uses a a scikit wrapper for the pytorch model\n",
    "#The model should have the following structure: hiddenlayers: 35, neuron per hidden layer: 110, optimizer: 'adamW', learning rate: 0.001, regularization:0.3\n",
    "#The activation function should be leaky relu for the hidden layers and linear for the output layer. The input layer should have 146 neurons and the output layer should have 1 neuron\n",
    "#The loss function is mse and the metric is mae, rmse and MAPE\n",
    "#use NeuralNetRegressor from skorch to build the model\n",
    "#epochs=150, batch_size=64\n",
    "\n",
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#load the data\n",
    "data = pd.read_csv('one_hot_encoded.csv')\n",
    "\n",
    "\n",
    "\n",
    "#split the data into features and target\n",
    "X = data.drop('claim', axis=1)\n",
    "\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "y = data['claim']\n",
    "\n",
    "#split the data into training and testing\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=69)\n",
    "# Second split: Split the 40% temporary set into 25% test and 15% evaluation\n",
    "X_eval, X_test, y_eval, y_test = train_test_split(X_temp, y_temp, test_size=(0.25/0.4), random_state=69)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "#standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_eval = scaler.transform(X_eval)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "#print datatypes\n",
    "print(X_train.dtype, y_train.dtype)\n",
    "#print the type of list\n",
    "print(type(X_train), type(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PyTorchModel.__init__() takes 1 positional argument but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m     64\u001b[0m regularization \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[1;32m---> 66\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPyTorchModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneurons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularization\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Define the loss function and optimizer\u001b[39;00m\n\u001b[0;32m     69\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()  \u001b[38;5;66;03m# Assuming this is a regression task, change if necessary\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:461\u001b[0m, in \u001b[0;36mModule.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.__init__() got an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_super_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(args):\n\u001b[1;32m--> 461\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.__init__() takes 1 positional argument but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(args)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m were\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m given\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mCalls super().__setattr__('a', a) instead of the typical self.a = a\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03mto avoid Module.__setattr__ overhead. Module's __setattr__ has special\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03mhandling for parameters, submodules, and buffers but simply calls into\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03msuper().__setattr__ for all other attributes.\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mTypeError\u001b[0m: PyTorchModel.__init__() takes 1 positional argument but 7 were given"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Define the model class\n",
    "class PyTorchModel(nn.Module):\n",
    "\n",
    "        \n",
    "    def __init__(self, input_dim, hidden_layers, neurons, optimizer, learning_rate, regularization):\n",
    "        super(PyTorchModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.neurons = neurons\n",
    "        self.optimizer = optimizer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(input_dim, neurons))\n",
    "        self.layers.append(nn.ReLU())\n",
    "        \n",
    "        for _ in range(hidden_layers - 1):\n",
    "            self.layers.append(nn.Linear(neurons, neurons))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(neurons, 1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Function to calculate RMSE\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "# Function to calculate MAE\n",
    "def calculate_mae(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "# Function to calculate MAPE\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) \n",
    "\n",
    "#convert y from pandas series to numpy array\n",
    "y_train = np.asarray(y_train)\n",
    "y_eval = np.asarray(y_eval)\n",
    "y_test = np.asarray(y_test)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(X_eval, dtype=torch.float32), torch.tensor(y_eval, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Create an instance of the PyTorch model\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_layers = 35\n",
    "neurons = 110\n",
    "optimizer_name = 'adamW'\n",
    "learning_rate = 0.001\n",
    "regularization = 0.01\n",
    "\n",
    "model = PyTorchModel(input_dim, hidden_layers, neurons, optimizer_name, learning_rate, regularization)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Assuming this is a regression task, change if necessary\n",
    "\n",
    "if optimizer_name.lower() == 'adam':\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
    "elif optimizer_name.lower() == 'adamw':\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=regularization)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported optimizer: {optimizer_name}\")\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 300\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0  # Initialize running_loss to zero at the start of each epoch\n",
    "\n",
    "    # Iterate over batches of data from train_loader\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels.view(-1, 1))  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        running_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader)}\")\n",
    "\n",
    "# Evaluate on the training set\n",
    "model.eval()\n",
    "train_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in train_loader:\n",
    "        outputs = model(inputs)\n",
    "        train_predictions.extend(outputs.numpy())\n",
    "train_predictions = np.array(train_predictions).flatten()\n",
    "y_train_flat = np.array(y_train).flatten()  # Flatten if necessary\n",
    "train_rmse = calculate_rmse(y_train_flat, train_predictions)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        test_predictions.extend(outputs.numpy())\n",
    "test_predictions = np.array(test_predictions).flatten()\n",
    "y_test_flat = np.array(y_test).flatten()  # Flatten if necessary\n",
    "test_rmse = calculate_rmse(y_test_flat, test_predictions)\n",
    "test_mae = calculate_mae(y_test_flat, test_predictions)\n",
    "test_mape = calculate_mape(y_test_flat, test_predictions)\n",
    "\n",
    "# Print RMSE, MAE, and MAPE values\n",
    "print(f\"Test RMSE: {test_rmse}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "print(f\"Test MAPE: {test_mape}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
